{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xADlklQwHM1j",
        "outputId": "25871eea-69ce-41fc-f44e-8bc1fa4ca152"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pinecone in /usr/local/lib/python3.10/dist-packages (5.3.1)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.0)\n",
            "Requirement already satisfied: langchain_pinecone in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (0.3.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (5.0.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone) (2024.8.30)\n",
            "Requirement already satisfied: pinecone-plugin-inference<2.0.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from pinecone) (1.1.0)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from pinecone) (0.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from pinecone) (2.8.2)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone) (2.0.7)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.2)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.0)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.125)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: pinecone-client<6.0.0,>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain_pinecone) (5.0.1)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.40.0 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (1.47.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.5.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.11.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.22.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain) (24.1)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (1.7.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (0.5.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.5.3->pinecone) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.9.11)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain-openai) (1.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Downloading langchain_openai-0.2.0-py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-openai\n",
            "Successfully installed langchain-openai-0.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pinecone langchain langchain_pinecone langchain-openai langchain-community pypdf python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "5LUhAAQaHNmT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import pinecone\n",
        "from pinecone import ServerlessSpec\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter # To split the text into smaller chunks\n",
        "from langchain_openai import OpenAIEmbeddings # To create embeddings\n",
        "from langchain_pinecone import PineconeVectorStore # To connect with the Vectorstore\n",
        "from langchain_community.document_loaders import DirectoryLoader # To load files in a directory\n",
        "from langchain_community.document_loaders import PyPDFLoader # To parse the PDFs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "iDGk-YuPHNou"
      },
      "outputs": [],
      "source": [
        "# os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "os.environ[\"PINECONE_API_KEY\"] = \"cbf01feb-d619-452b-a8a5-b4f5a4a7e9ae\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7KOq15dHNrC",
        "outputId": "e8e41296-8c0a-428e-d0be-3f8a2af9d713"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<pinecone.control.pinecone.Pinecone at 0x7eedddfbd780>"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "index_name = \"test\" #give the name to your index, or you can use an index which you created previously and load that.\n",
        "#here we are using the new fresh index name\n",
        "pc = Pinecone(api_key=\"cbf01feb-d619-452b-a8a5-b4f5a4a7e9ae\")\n",
        "#Get your Pinecone API key to connect after successful login and put it here.\n",
        "pc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7b9UcNpHNtb",
        "outputId": "a749c3fc-f13f-4552-b093-6499e01653e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "index already exists test\n",
            "{'dimension': 1536,\n",
            " 'index_fullness': 0.0,\n",
            " 'namespaces': {},\n",
            " 'total_vector_count': 0}\n",
            "index created\n",
            "{'dimension': 1536,\n",
            " 'index_fullness': 0.0,\n",
            " 'namespaces': {},\n",
            " 'total_vector_count': 0}\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "if index_name in pc.list_indexes().names():\n",
        "  print(\"index already exists\" , index_name)\n",
        "  index= pc.Index(index_name) #your index which is already existing and is ready to use\n",
        "  print(index.describe_index_stats())\n",
        "\n",
        "else: #crate a new index with specs\n",
        "  pc.create_index(\n",
        "  name=index_name,\n",
        "  dimension=1536, # Replace with your model dimensions\n",
        "  metric=\"cosine\", # Replace with your model metric\n",
        "  spec=ServerlessSpec(\n",
        "  cloud=\"aws\",\n",
        "  region=\"us-east-1\")\n",
        ")\n",
        "\n",
        "while not pc.describe_index(index_name).status[\"ready\"]:\n",
        "  time.sleep(1)\n",
        "index= pc.Index(index_name)\n",
        "print(\"index created\")\n",
        "print(index.describe_index_stats())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "WQ9aEWgoHNwH"
      },
      "outputs": [],
      "source": [
        "DATA_DIR_PATH = \"/content/pdfs\" # Directory containing our PDF files\n",
        "CHUNK_SIZE = 1024 # Size of each text chunk for processing\n",
        "CHUNK_OVERLAP = 0 # Amount of overlap between chunks\n",
        "INDEX_NAME = index_name # Name of our Pinecone index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIGRl3ddHNy9",
        "outputId": "95912081-a75e-4bca-e2c6-4b69c052f99d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Documents loaded: 18\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
        "loader = DirectoryLoader(\n",
        "    path=DATA_DIR_PATH, # Directory containing our PDFs\n",
        "    glob=\"**/*.pdf\", # Pattern to match PDF files (including subdirectories)\n",
        "    loader_cls=PyPDFLoader # Specifies we're loading PDF files\n",
        ")\n",
        "docs = loader.load()# This loads all matching PDF files\n",
        "print(f\"Total Documents loaded: {len(docs)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32E3XDGSHN1w",
        "outputId": "77230e6c-0b25-4759-d625-b46c034bb588"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'source': '/content/pdfs/rachelgreecv.pdf', 'page': 0}, page_content='3 grad.illinois.edu/CareerDevelopment Rachel Green  \\n2 1 0  W .  G R E E N  S T . ,  C H A M P A I G N ,  I L  \\n( 2 1 7 )  5 5 5 - 1 2 3 4  •  R S T U D E N T @ I L L I N O I S . E D U  \\nEDUCATION  \\nPhD in English May 20xx \\nUniversity of Illinois at Urbana-Champaign \\nDissertation title:  “Down on the Farm: World War One and the Emergence of Literary  \\nModernism in the American South”  \\nCommittee : Margaret Black, Naomi Blue, John Jay, Robert Roberts (Chair) \\nMA in English  20xx \\nUniversity of Illinois at Urbana-Champaign \\nBA in English and Communications, summa cum laude  20xx \\nButler University, Indianapolis, IN  \\nTEACHING  & A DVISING   \\nComposition Instructor  20xx-present \\nResearch Writing Program, University of Illinois \\n\\uf0b7Facilitator for seven sections of English composition.\\n\\uf0b7Planned and taught a writing-intensive course based upon current events.\\n\\uf0b7Used instructional technology to enhance pedagogical technique.\\n\\uf0b7Taught in part with an innovative, interdisciplinary team-teaching program design.\\nLiterature Instructor 20xx-present \\nDepartment of English, University of Illinois \\n\\uf0b7Instructor of record for two sections of literature, including Major American Authors  and\\nIntroduction to Poetry per semester.\\n\\uf0b7Integrated multimedia and humanities approaches to teaching literature using film and instructional\\ntechnology.\\nCoordinating Group Leader 20xx-20xx \\nResearch Writing Program, University of Illinois \\n\\uf0b7Planned and led required training session for teaching assistants and new composition teachers.\\n\\uf0b7Helped to mentor new hires to the English Department staff to ensure their engagement and\\nprofessional development.\\n\\uf0b7Provided job shadowing and training opportunities to assist new hires in adjusting to the pace of\\nwork and the tone and style of the University.\\nDiscussion Leader  20xx \\nCarolina Summer Reading Program, University of Illinois  \\n\\uf0b7Led group discussion for first-year students on academic topics.\\nTeaching Assistant 20xx-20xx \\nDepartment of English, University of Illinois at Urbana-Champaign \\n\\uf0b7Taught a section on film criticism, including film history, theory and technical vocabulary.\\n\\uf0b7Planned lessons and assignments, led discussion sections, graded papers and exams.\\n\\uf0b7Organized and led group discussions on social and academic issues.CV SAMPLE ')"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "v1t5OE9jIk8I",
        "outputId": "f2b0dfa3-b3d3-462c-cffb-516f42493abd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_core.documents.base.Document</b><br/>def __init__(page_content: str, **kwargs: Any) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/langchain_core/documents/base.py</a>Class for storing a piece of text and associated metadata.\n",
              "\n",
              "Example:\n",
              "\n",
              "    .. code-block:: python\n",
              "\n",
              "        from langchain_core.documents import Document\n",
              "\n",
              "        document = Document(\n",
              "            page_content=&quot;Hello, world!&quot;,\n",
              "            metadata={&quot;source&quot;: &quot;https://example.com&quot;}\n",
              "        )</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 258);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ],
            "text/plain": [
              "langchain_core.documents.base.Document"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(docs[14])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lB7brYQVIoRQ",
        "outputId": "ec243f5b-6c56-4db2-98ae-4bc3dbd0bfbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "keys associated with a Document: dict_keys(['id', 'metadata', 'page_content', 'type'])\n"
          ]
        }
      ],
      "source": [
        "# we can convert the Document object to a python dict using the .dict() method.\n",
        "print(f\"keys associated with a Document: {docs[0].dict().keys()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoXLCkjjItNX",
        "outputId": "b56fc1b3-9dea-4c76-fe18-40982c2962df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------\n",
            "First 100 charachters of the page content: 3 grad.illinois.edu/CareerDevelopment Rachel Green  \n",
            "2 1 0  W .  G R E E N  S T . ,  C H A M P A I G\n",
            "---------------\n",
            "Metadata associated with the document: {'source': '/content/pdfs/rachelgreecv.pdf', 'page': 0}\n",
            "---------------\n",
            "Datatype of the document: Document\n",
            "---------------\n"
          ]
        }
      ],
      "source": [
        "print(f\"{'-'*15}\\nFirst 100 charachters of the page content: {docs[0].page_content[:100]}\\n{'-'*15}\")\n",
        "print(f\"Metadata associated with the document: {docs[0].metadata}\\n{'-'*15}\")\n",
        "print(f\"Datatype of the document: {docs[0].type}\\n{'-'*15}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7cy6bruIu_Y",
        "outputId": "c5e65bf5-9b7e-472d-9de9-3b2a8222f360"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metadata associated with the document: {'filename': '/content/pdfs/rachelgreecv.pdf', 'source': '/content/pdfs/rachelgreecv.pdf', 'page': 0}\n",
            "---------------\n",
            "Metadata associated with the document: {'filename': '/content/pdfs/rachelgreecv.pdf', 'source': '/content/pdfs/rachelgreecv.pdf', 'page': 1}\n",
            "---------------\n",
            "Metadata associated with the document: {'filename': '/content/pdfs/rachelgreecv.pdf', 'source': '/content/pdfs/rachelgreecv.pdf', 'page': 2}\n",
            "---------------\n",
            "Metadata associated with the document: {'filename': '/content/pdfs/yolov7paper.pdf', 'source': '/content/pdfs/yolov7paper.pdf', 'page': 0}\n",
            "---------------\n"
          ]
        }
      ],
      "source": [
        "#  We loop through each document and add additional metadata - filename, quarter, and year\n",
        "for doc in docs:\n",
        "  filename = doc.dict()['metadata']['source'].split(\"\\\\\")[-1]\n",
        "  #quarter = doc.dict()['metadata']['source'].split(\"\\\\\")[-2]\n",
        "  #year = doc.dict()['metadata']['source'].split(\"\\\\\")[-3]\n",
        "  doc.metadata = {\"filename\": filename, \"source\": doc.dict()['metadata']['source'], \"page\": doc.dict()['metadata']['page']}\n",
        "\n",
        "# To veryfy that the metadata is indeed added to the document\n",
        "print(f\"Metadata associated with the document: {docs[0].metadata}\\n{'-'*15}\")\n",
        "print(f\"Metadata associated with the document: {docs[1].metadata}\\n{'-'*15}\")\n",
        "print(f\"Metadata associated with the document: {docs[2].metadata}\\n{'-'*15}\")\n",
        "print(f\"Metadata associated with the document: {docs[3].metadata}\\n{'-'*15}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKyG3P93I1Dw",
        "outputId": "674f6a3f-af03-4da1-9bb3-3c031a932039"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metadata associated with the document: {'filename': '/content/pdfs/rachelgreecv.pdf', 'source': '/content/pdfs/rachelgreecv.pdf', 'page': 0}\n",
            "---------------\n",
            "Metadata associated with the document: {'filename': '/content/pdfs/rachelgreecv.pdf', 'source': '/content/pdfs/rachelgreecv.pdf', 'page': 1}\n",
            "---------------\n",
            "Metadata associated with the document: {'filename': '/content/pdfs/rachelgreecv.pdf', 'source': '/content/pdfs/rachelgreecv.pdf', 'page': 2}\n",
            "---------------\n",
            "Metadata associated with the document: {'filename': '/content/pdfs/yolov7paper.pdf', 'source': '/content/pdfs/yolov7paper.pdf', 'page': 0}\n",
            "---------------\n",
            "Metadata associated with the document: {'filename': '/content/pdfs/yolov7paper.pdf', 'source': '/content/pdfs/yolov7paper.pdf', 'page': 1}\n",
            "---------------\n",
            "Metadata associated with the document: {'filename': '/content/pdfs/yolov7paper.pdf', 'source': '/content/pdfs/yolov7paper.pdf', 'page': 2}\n",
            "---------------\n",
            "Metadata associated with the document: {'filename': '/content/pdfs/yolov7paper.pdf', 'source': '/content/pdfs/yolov7paper.pdf', 'page': 3}\n",
            "---------------\n",
            "Metadata associated with the document: {'filename': '/content/pdfs/yolov7paper.pdf', 'source': '/content/pdfs/yolov7paper.pdf', 'page': 4}\n",
            "---------------\n",
            "Metadata associated with the document: {'filename': '/content/pdfs/yolov7paper.pdf', 'source': '/content/pdfs/yolov7paper.pdf', 'page': 5}\n",
            "---------------\n",
            "Metadata associated with the document: {'filename': '/content/pdfs/yolov7paper.pdf', 'source': '/content/pdfs/yolov7paper.pdf', 'page': 6}\n",
            "---------------\n",
            "Metadata associated with the document: {'filename': '/content/pdfs/yolov7paper.pdf', 'source': '/content/pdfs/yolov7paper.pdf', 'page': 7}\n",
            "---------------\n",
            "Metadata associated with the document: {'filename': '/content/pdfs/yolov7paper.pdf', 'source': '/content/pdfs/yolov7paper.pdf', 'page': 8}\n",
            "---------------\n",
            "Metadata associated with the document: {'filename': '/content/pdfs/yolov7paper.pdf', 'source': '/content/pdfs/yolov7paper.pdf', 'page': 9}\n",
            "---------------\n",
            "Metadata associated with the document: {'filename': '/content/pdfs/yolov7paper.pdf', 'source': '/content/pdfs/yolov7paper.pdf', 'page': 10}\n",
            "---------------\n",
            "Metadata associated with the document: {'filename': '/content/pdfs/yolov7paper.pdf', 'source': '/content/pdfs/yolov7paper.pdf', 'page': 11}\n",
            "---------------\n",
            "Metadata associated with the document: {'filename': '/content/pdfs/yolov7paper.pdf', 'source': '/content/pdfs/yolov7paper.pdf', 'page': 12}\n",
            "---------------\n",
            "Metadata associated with the document: {'filename': '/content/pdfs/yolov7paper.pdf', 'source': '/content/pdfs/yolov7paper.pdf', 'page': 13}\n",
            "---------------\n",
            "Metadata associated with the document: {'filename': '/content/pdfs/yolov7paper.pdf', 'source': '/content/pdfs/yolov7paper.pdf', 'page': 14}\n",
            "---------------\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(docs)) :\n",
        "  print(f\"Metadata associated with the document: {docs[i].metadata}\\n{'-'*15}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "--IQ1YfiI2uw"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "  chunk_size=1024,\n",
        "  chunk_overlap=0\n",
        ")\n",
        "documents = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08JyyvP0I53I",
        "outputId": "04a3f18b-0972-4792-b3ac-e4ce751e5184"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(25, 118)"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Split text into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "  chunk_size=CHUNK_SIZE,\n",
        "  chunk_overlap=CHUNK_OVERLAP\n",
        ")\n",
        "documents = text_splitter.split_documents(docs)\n",
        "len(docs), len(documents)\n",
        "#output ;\n",
        "(25, 118)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bu-aApynI76v",
        "outputId": "dbe02b26-87c7-49d8-cf1b-a22b3f91f34a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x7eedd06f02b0>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x7eedd06f1690>, model='text-embedding-ada-002', dimensions=None, deployment='text-embedding-ada-002', openai_api_version=None, openai_api_base=None, openai_api_type=None, openai_proxy=None, embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True)"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embeddings = OpenAIEmbeddings(model = \"text-embedding-ada-002\") # Initialize the embedding model\n",
        "embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-7jscqOI9zA",
        "outputId": "88331fd5-ab32-4cf5-d630-4b51c6a31fe1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Are the vectors already added in DB: (Type Y/N)N\n",
            "New vectorstore is created and loaded\n"
          ]
        }
      ],
      "source": [
        "docs_already_in_pinecone = input(\"Are the vectors already added in DB: (Type Y/N)\")\n",
        "# check if the documents were already added to the vector database\n",
        "if docs_already_in_pinecone == \"Y\" or docs_already_in_pinecone == \"y\":\n",
        "  docsearch = PineconeVectorStore(index_name=INDEX_NAME, embedding=embeddings)\n",
        "  print(\"Existing Vectorstore is loaded\")\n",
        "# if not then add the documents to the vectore db\n",
        "elif docs_already_in_pinecone == \"N\" or docs_already_in_pinecone == \"n\":\n",
        "  docsearch = PineconeVectorStore.from_documents(documents, embeddings, index_name=index_name)\n",
        "  print(\"New vectorstore is created and loaded\")\n",
        "else:\n",
        "  print(\"Please type Y - for yes and N - for no\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNKsZRcyJBw4",
        "outputId": "8cb39f0f-022f-4e90-91aa-feb40b849cb3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id='47a095b0-cfc4-4478-931d-5dc8f242f932', metadata={'filename': '/content/pdfs/yolov7paper.pdf', 'page': 11.0, 'source': '/content/pdfs/yolov7paper.pdf'}, page_content='[23] Jocher Glenn. YOLOv5 release v6.1. https://github.com/\\nultralytics/yolov5/releases/tag/v6.1, 2022. 2, 7, 10\\n[24] Shuxuan Guo, Jose M Alvarez, and Mathieu Salzmann. Ex-\\npandNets: Linear over-parameterization to train compact\\nconvolutional networks. Advances in Neural Information\\nProcessing Systems (NeurIPS) , 33:1298–1310, 2020. 2\\n[25] Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing\\nXu, and Chang Xu. GhostNet: More features from cheap\\noperations. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n1580–1589, 2020. 1\\n[26] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nDeep residual learning for image recognition. In Proceed-\\n12'),\n",
              " Document(id='05fc01f2-1393-4ea2-8c2b-65ecec955741', metadata={'filename': '/content/pdfs/yolov7paper.pdf', 'page': 6.0, 'source': '/content/pdfs/yolov7paper.pdf'}, page_content='YOLOv7-W6 70.4M 360.0G 1280 84 54.9% / 54.6% 72.6% 60.1% 37.3% 58.7% 67.1%\\nYOLOv7-E6 97.2M 515.2G 1280 56 56.0% / 55.9% 73.5% 61.2% 38.0% 59.9% 68.4%\\nYOLOv7-D6 154.7M 806.8G 1280 44 56.6% / 56.3% 74.0% 61.8% 38.8% 60.1% 69.5%\\nYOLOv7-E6E 151.7M 843.2G 1280 36 56.8% / 56.8% 74.4% 62.1% 39.3% 60.5% 69.0%\\n1Our FLOPs is calaculated by rectangle input resolution like 640 ×640 or 1280 ×1280.\\n2Our inference time is estimated by using letterbox resize input image to make its long side equals to 640 or 1280.\\n5.2. Baselines\\nWe choose previous version of YOLO [3, 79] and state-\\nof-the-art object detector YOLOR [81] as our baselines. Ta-\\nble 1 shows the comparison of our proposed YOLOv7 mod-\\nels and those baseline that are trained with the same settings.\\nFrom the results we see that if compared with YOLOv4,\\nYOLOv7 has 75% less parameters, 36% less computation,\\nand brings 1.5% higher AP. If compared with state-of-the-\\nart YOLOR-CSP, YOLOv7 has 43% fewer parameters, 15%'),\n",
              " Document(id='5a2b27cc-701c-4b3d-b28b-85eeec3fae71', metadata={'filename': '/content/pdfs/yolov7paper.pdf', 'page': 0.0, 'source': '/content/pdfs/yolov7paper.pdf'}, page_content='YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object\\ndetectors\\nChien-Yao Wang1, Alexey Bochkovskiy, and Hong-Yuan Mark Liao1\\n1Institute of Information Science, Academia Sinica, Taiwan\\nkinyiu@iis.sinica.edu.tw, alexeyab84@gmail.com, and liao@iis.sinica.edu.tw\\nAbstract\\nYOLOv7 surpasses all known object detectors in both\\nspeed and accuracy in the range from 5 FPS to 160 FPS\\nand has the highest accuracy 56.8% AP among all known\\nreal-time object detectors with 30 FPS or higher on GPU\\nV100. YOLOv7-E6 object detector (56 FPS V100, 55.9%\\nAP) outperforms both transformer-based detector SWIN-\\nL Cascade-Mask R-CNN (9.2 FPS A100, 53.9% AP) by\\n509% in speed and 2% in accuracy, and convolutional-\\nbased detector ConvNeXt-XL Cascade-Mask R-CNN (8.6\\nFPS A100, 55.2% AP) by 551% in speed and 0.7% AP\\nin accuracy, as well as YOLOv7 outperforms: YOLOR,\\nYOLOX, Scaled-YOLOv4, YOLOv5, DETR, Deformable\\nDETR, DINO-5scale-R50, ViT-Adapter-B and many other'),\n",
              " Document(id='0a593530-aaaf-4683-b7f8-2bc5f04f8a24', metadata={'filename': '/content/pdfs/yolov7paper.pdf', 'page': 5.0, 'source': '/content/pdfs/yolov7paper.pdf'}, page_content='tain YOLOv7-X. As for YOLOv7-W6, we use the newly\\nproposed compound scaling method to obtain YOLOv7-E6\\nand YOLOv7-D6. In addition, we use the proposed E-\\nELAN for YOLOv7-E6, and thereby complete YOLOv7-\\nE6E. Since YOLOv7-tiny is an edge GPU-oriented archi-\\ntecture, it will use leaky ReLU as activation function. As\\nfor other models we use SiLU as activation function. We\\nwill describe the scaling factor of each model in detail in\\nAppendix.\\n6')]"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Using the Vector Store for Retrieval\n",
        "# Here we are defing how to use the loaded vectorstore as retriver\n",
        "retriver = docsearch.as_retriever()\n",
        "retriver.invoke(\"what is yolo?\")#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORd0zy_nJMxP",
        "outputId": "4bbab692-cda2-43f6-b75a-88628b314d1a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id='df94cf4f-a7be-43b3-a64b-b0e9144ed0d3', metadata={'filename': '/content/pdfs/yolov7paper.pdf', 'page': 9.0, 'source': '/content/pdfs/yolov7paper.pdf'}, page_content='YOLOv7 surpasses all known object detectors in both\\nspeed and accuracy in the range from 5 FPS to 160 FPS and\\nhas the highest accuracy 56.8% AP test-dev / 56.8% AP\\nmin-val among all known real-time object detectors with 30\\nFPS or higher on GPU V100. YOLOv7-E6 object detector\\n(56 FPS V100, 55.9% AP) outperforms both transformer-\\nbased detector SWIN-L Cascade-Mask R-CNN (9.2 FPS\\nA100, 53.9% AP) by 509% in speed and 2% in accuracy,and convolutional-based detector ConvNeXt-XL Cascade-\\nMask R-CNN (8.6 FPS A100, 55.2% AP) by 551% in speed\\nand 0.7% AP in accuracy, as well as YOLOv7 outperforms:\\nYOLOR, YOLOX, Scaled-YOLOv4, YOLOv5, DETR, De-\\nformable DETR, DINO-5scale-R50, ViT-Adapter-B and\\nmany other object detectors in speed and accuracy. More\\nover, we train YOLOv7 only on MS COCO dataset from\\nscratch without using any other datasets or pre-trained\\nweights.\\n10'),\n",
              " Document(id='05fc01f2-1393-4ea2-8c2b-65ecec955741', metadata={'filename': '/content/pdfs/yolov7paper.pdf', 'page': 6.0, 'source': '/content/pdfs/yolov7paper.pdf'}, page_content='YOLOv7-W6 70.4M 360.0G 1280 84 54.9% / 54.6% 72.6% 60.1% 37.3% 58.7% 67.1%\\nYOLOv7-E6 97.2M 515.2G 1280 56 56.0% / 55.9% 73.5% 61.2% 38.0% 59.9% 68.4%\\nYOLOv7-D6 154.7M 806.8G 1280 44 56.6% / 56.3% 74.0% 61.8% 38.8% 60.1% 69.5%\\nYOLOv7-E6E 151.7M 843.2G 1280 36 56.8% / 56.8% 74.4% 62.1% 39.3% 60.5% 69.0%\\n1Our FLOPs is calaculated by rectangle input resolution like 640 ×640 or 1280 ×1280.\\n2Our inference time is estimated by using letterbox resize input image to make its long side equals to 640 or 1280.\\n5.2. Baselines\\nWe choose previous version of YOLO [3, 79] and state-\\nof-the-art object detector YOLOR [81] as our baselines. Ta-\\nble 1 shows the comparison of our proposed YOLOv7 mod-\\nels and those baseline that are trained with the same settings.\\nFrom the results we see that if compared with YOLOv4,\\nYOLOv7 has 75% less parameters, 36% less computation,\\nand brings 1.5% higher AP. If compared with state-of-the-\\nart YOLOR-CSP, YOLOv7 has 43% fewer parameters, 15%'),\n",
              " Document(id='5a2b27cc-701c-4b3d-b28b-85eeec3fae71', metadata={'filename': '/content/pdfs/yolov7paper.pdf', 'page': 0.0, 'source': '/content/pdfs/yolov7paper.pdf'}, page_content='YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object\\ndetectors\\nChien-Yao Wang1, Alexey Bochkovskiy, and Hong-Yuan Mark Liao1\\n1Institute of Information Science, Academia Sinica, Taiwan\\nkinyiu@iis.sinica.edu.tw, alexeyab84@gmail.com, and liao@iis.sinica.edu.tw\\nAbstract\\nYOLOv7 surpasses all known object detectors in both\\nspeed and accuracy in the range from 5 FPS to 160 FPS\\nand has the highest accuracy 56.8% AP among all known\\nreal-time object detectors with 30 FPS or higher on GPU\\nV100. YOLOv7-E6 object detector (56 FPS V100, 55.9%\\nAP) outperforms both transformer-based detector SWIN-\\nL Cascade-Mask R-CNN (9.2 FPS A100, 53.9% AP) by\\n509% in speed and 2% in accuracy, and convolutional-\\nbased detector ConvNeXt-XL Cascade-Mask R-CNN (8.6\\nFPS A100, 55.2% AP) by 551% in speed and 0.7% AP\\nin accuracy, as well as YOLOv7 outperforms: YOLOR,\\nYOLOX, Scaled-YOLOv4, YOLOv5, DETR, Deformable\\nDETR, DINO-5scale-R50, ViT-Adapter-B and many other')]"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"YOLOv7 outperforms which models\"\n",
        "\n",
        "docs = docsearch.similarity_search(query, k=3)\n",
        "\n",
        "docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJWdR98DJfeP",
        "outputId": "ff0c8b5b-c24e-4e39-da73-8133b002aef5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-63-4ab60154b5e1>:1: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
            "  llm = OpenAI()\n"
          ]
        }
      ],
      "source": [
        "llm = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "XNSNkN7dJ3h3"
      },
      "outputs": [],
      "source": [
        "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "eiGuU78JJ5G3"
      },
      "outputs": [],
      "source": [
        "query = \"YOLOv7 outperforms which models\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "fl_bXVlaJ6oW",
        "outputId": "0780634b-2616-489f-c927-d470a362e7bf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-66-fbc3ad05536a>:1: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  qa.run(query)\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' YOLOR, YOLOX, Scaled-YOLOv4, YOLOv5, DETR, Deformable DETR, DINO-5scale-R50, ViT-Adapter-B and many other object detectors.'"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qa.run(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "SfIkRr2WJ8W3",
        "outputId": "6b9fd61e-772b-424f-80af-986f53a6ed0a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Rachel Green has experience as a composition instructor and has received various awards and honors for her academic achievements. She has also presented at several conferences and has publications in academic journals and books.'"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"Rachel Green Experience\"\n",
        "qa.run(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "Ls9nodriKASe",
        "outputId": "ba93146f-aa4d-41ed-b569-85310110bc58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input Prompt: what is yolo v7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-68-542f9053024a>:10: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  result = qa({'query': user_input})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer:  YOLOv7 is a real-time object detection model that has surpassed all known object detectors in both speed and accuracy in the range from 5 FPS to 160 FPS. It has the highest accuracy of 56.8% AP among all known real-time object detectors with 30 FPS or higher on GPU V100. It is a trainable bag-of-freebies that sets a new state-of-the-art for real-time object detectors.\n",
            "Input Prompt: exit\n",
            "Exiting\n"
          ]
        },
        {
          "ename": "SystemExit",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "while True:\n",
        "  user_input = input(f\"Input Prompt: \")\n",
        "  if user_input == 'exit':\n",
        "    print('Exiting')\n",
        "    sys.exit()\n",
        "  if user_input == '':\n",
        "    continue\n",
        "  result = qa({'query': user_input})\n",
        "  print(f\"Answer: {result['result']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOKCFB6RKEXe"
      },
      "outputs": [],
      "source": [
        "# https://www.analyticsvidhya.com/blog/2024/06/pinecone-vector-databases/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
